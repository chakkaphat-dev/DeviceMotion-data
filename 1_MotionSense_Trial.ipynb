{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_MotionSense_Trial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOPX2kFU+Bina0jzYjW6z1c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomMaaiGG/DeviceMotion-data/blob/main/1_MotionSense_Trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYwHqmYaywWz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "##_____________________________\n",
        "\n",
        "def get_ds_infos():\n",
        "    ## 0:Code, 1:Weight, 2:Height, 3:Age, 4:Gender\n",
        "    dss = np.genfromtxt(\"data_subjects_info.csv\",delimiter=',')\n",
        "    dss = dss[1:]\n",
        "    print(\"----> Data subjects information is imported.\")\n",
        "    return dss\n",
        "##____________\n",
        "\n",
        "def creat_time_series(num_features, num_act_labels, num_gen_labels, label_codes, trial_codes):\n",
        "    dataset_columns = num_features+num_act_labels+num_gen_labels\n",
        "    ds_list = get_ds_infos()\n",
        "    train_data = np.zeros((0,dataset_columns))\n",
        "    test_data = np.zeros((0,dataset_columns))\n",
        "    for i, sub_id in enumerate(ds_list[:,0]):\n",
        "        for j, act in enumerate(label_codes):\n",
        "            for trial in trial_codes[act]:\n",
        "                fname = 'A_DeviceMotion_data/'+act+'_'+str(trial)+'/sub_'+str(int(sub_id))+'.csv'\n",
        "                raw_data = pd.read_csv(fname)\n",
        "                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n",
        "                unlabel_data = raw_data.values\n",
        "                label_data = np.zeros((len(unlabel_data), dataset_columns))\n",
        "                label_data[:,:-(num_act_labels + num_gen_labels)] = unlabel_data\n",
        "                label_data[:,label_codes[act]] = 1\n",
        "                label_data[:,-(num_gen_labels)] = int(ds_list[i,4])\n",
        "                ## We consider long trials as training dataset and short trials as test dataset\n",
        "                if trial > 10:\n",
        "                    test_data = np.append(test_data, label_data, axis = 0)\n",
        "                else:    \n",
        "                    train_data = np.append(train_data, label_data, axis = 0)\n",
        "    return train_data , test_data\n",
        "#________________________________\n",
        "\n",
        "\n",
        "print(\"--> Start...\")\n",
        "## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n",
        "num_features = 12 # attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\n",
        "num_act_labels = 4 # dws, ups, wlk, jog\n",
        "num_gen_labels = 1 # 0/1(female/male)\n",
        "label_codes = {\"dws\":num_features, \"ups\":num_features+1, \"wlk\":num_features+2, \"jog\":num_features+3}\n",
        "trial_codes = {\"dws\":[1,2,11], \"ups\":[3,4,12], \"wlk\":[7,8,15], \"jog\":[9,16]}    \n",
        "## Calling 'creat_time_series()' to build time-series\n",
        "print(\"--> Building Training and Test Datasets...\")\n",
        "train_ts, test_ts = creat_time_series(num_features, num_act_labels, num_gen_labels, label_codes, trial_codes)\n",
        "print(\"--> Shape of Training Time-Seires:\", train_ts.shape)\n",
        "print(\"--> Shape of Test Time-Series:\", test_ts.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_series_to_section(dataset, num_act_labels, num_gen_labels, sliding_window_size, step_size_of_sliding_window, standardize = False, **options):\n",
        "    data = dataset[: , 0:-(num_act_labels+num_gen_labels)]\n",
        "    act_labels = dataset[: , -(num_act_labels+num_gen_labels):-(num_gen_labels)]\n",
        "    gen_labels = dataset[: , -(num_gen_labels)]\n",
        "    mean = 0\n",
        "    std = 1\n",
        "    \n",
        "    if standardize:\n",
        "        ## Standardize each sensorâ€™s data to have a zero mean and unity standard deviation.\n",
        "        ## As usual, we normalize test dataset by training dataset's parameters \n",
        "        if options:\n",
        "            mean = options.get(\"mean\")\n",
        "            std = options.get(\"std\")\n",
        "            print(\"----> Test Data has been standardized\")\n",
        "        else:\n",
        "            mean = data.mean(axis=0)\n",
        "            std = data.std(axis=0)\n",
        "            print(\"----> Training Data has been standardized:\\n the mean is = \",str(mean.mean()),\" ; and the std is = \",str(std.mean()))            \n",
        "  \n",
        "        data -= mean\n",
        "        data /= std\n",
        "    else:\n",
        "        print(\"----> Without Standardization.....\")\n",
        "\n",
        "    ## We want the Rows of matrices show each Feature and the Columns show time points.\n",
        "    data = data.T\n",
        "            \n",
        "    size_features = data.shape[0]\n",
        "    size_data = data.shape[1]\n",
        "    number_of_secs = round(((size_data - sliding_window_size)/step_size_of_sliding_window))\n",
        "            \n",
        "    ##  Create a 3D matrix for Storing Snapshots  \n",
        "    secs_data = np.zeros((number_of_secs , size_features , sliding_window_size ))\n",
        "    act_secs_labels = np.zeros((number_of_secs, num_act_labels))\n",
        "    gen_secs_labels = np.zeros(number_of_secs)\n",
        "    \n",
        "    k=0    \n",
        "    for i in range(0 ,(size_data)-sliding_window_size  , step_size_of_sliding_window):\n",
        "        j = i // step_size_of_sliding_window\n",
        "        if(j>=number_of_secs):\n",
        "            break\n",
        "        if(gen_labels[i] != gen_labels[i+sliding_window_size-1]): \n",
        "            continue\n",
        "        if(not (act_labels[i] == act_labels[i+sliding_window_size-1]).all()): \n",
        "            continue    \n",
        "        secs_data[k] = data[0:size_features, i:i+sliding_window_size]\n",
        "        act_secs_labels[k] = act_labels[i].astype(int)\n",
        "        gen_secs_labels[k] = gen_labels[i].astype(int)\n",
        "        k = k+1\n",
        "    secs_data = secs_data[0:k]\n",
        "    act_secs_labels = act_secs_labels[0:k]\n",
        "    gen_secs_labels = gen_secs_labels[0:k]\n",
        "    \n",
        "    return secs_data, act_secs_labels, gen_secs_labels, mean, std\n",
        "##________________________________________________________________\n",
        "\n",
        "\n",
        "## This Variable Defines the Size of Sliding Window\n",
        "## ( e.g. 100 means in each snapshot we just consider 100 consecutive observations of each sensor) \n",
        "sliding_window_size = 50 # 50 Equals to 1 second for MotionSense Dataset (it is on 50Hz samplig rate)\n",
        "## Here We Choose Step Size for Building Diffrent Snapshots from Time-Series Data\n",
        "## ( smaller step size will increase the amount of the instances and higher computational cost may be incurred )\n",
        "step_size_of_sliding_window = 10 \n",
        "print(\"--> Sectioning Training and Test datasets: shape of each section will be: (\",num_features,\"x\",sliding_window_size,\")\")\n",
        "train_data, act_train_labels, gen_train_labels, train_mean, train_std = time_series_to_section(train_ts.copy(),\n",
        "                                                                                               num_act_labels,\n",
        "                                                                                               num_gen_labels,\n",
        "                                                                                               sliding_window_size,\n",
        "                                                                                               step_size_of_sliding_window,\n",
        "                                                                                               standardize = True)\n",
        "\n",
        "test_data, act_test_labels, gen_test_labels, test_mean, test_std = time_series_to_section(test_ts.copy(),\n",
        "                                                                                          num_act_labels,\n",
        "                                                                                          num_gen_labels,\n",
        "                                                                                          sliding_window_size,\n",
        "                                                                                          step_size_of_sliding_window,\n",
        "                                                                                          standardize = True,\n",
        "                                                                                          mean = train_mean, \n",
        "                                                                                          std = train_std)\n",
        "print(\"--> Shape of Training Sections:\", train_data.shape)\n",
        "print(\"--> Shape of Test Sections:\", test_data.shape)"
      ],
      "metadata": {
        "id": "BVU8DemTy5H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.models import Model \n",
        "from keras.layers import Input, Dense, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
        "from keras.utils import np_utils \n",
        "##______________________________\n",
        "## Here we add an extra dimension to the datasets just to be ready for using with Convolution2D\n",
        "train_data = np.expand_dims(train_data,axis=3)\n",
        "test_data = np.expand_dims(test_data,axis=3)\n",
        "print(\"--> Shape of Training Sections:\", train_data.shape)\n",
        "print(\"--> Shape of Test Sections:\", test_data.shape)\n"
      ],
      "metadata": {
        "id": "KtFe6lwYy6Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##***@@@ This Will Be the ESTIMATOR @@@***##\n",
        "## Here we set up the parameters for MTCNN\n",
        "num_train, height, width, channel = train_data.shape\n",
        "metrics = ['acc']\n",
        "## Activity Recognition\n",
        "act_last_layer_dim = num_act_labels\n",
        "act_loss_func = \"categorical_crossentropy\"\n",
        "act_activation_func = 'softmax'\n",
        "## Gender Classification\n",
        "gen_last_layer_dim = num_gen_labels \n",
        "gen_loss_func = \"binary_crossentropy\"\n",
        "gen_activation_func = 'sigmoid'\n",
        "## Training Phase\n",
        "batch_size = 64\n",
        "num_of_epochs = 20\n",
        "verbosity = 1\n",
        "## MTCNN\n",
        "kernel_size_1 = 5\n",
        "kernel_size_2 = 3\n",
        "pool_size_1 = 2\n",
        "pool_size_2 = 3  \n",
        "conv_depth_1 = 50 \n",
        "conv_depth_2 = 40 \n",
        "conv_depth_3 = 20 \n",
        "drop_prob_1 = 0.2 \n",
        "drop_prob_2 = 0.4 \n",
        "hidden_size = 400 \n",
        "\n",
        "## Note that: because each section of time-series is a matrix, we use Convolution2D.\n",
        "## On the other side: because each row of the matrix correspond to one feature of\n",
        "##   time-series, so we use a (1,k) kernel to convolve the data points of each row with \n",
        "##   just that row's data points\n",
        "inp = Input(shape=(height, width,1))     \n",
        "conv_0 = Convolution2D(conv_depth_1, (1 , kernel_size_1), padding='valid', activation='relu')(inp)\n",
        "conv_1 = Convolution2D(conv_depth_1, (1 , kernel_size_2), padding='same', activation='relu')(conv_0)\n",
        "dense_1 = Dense(conv_depth_1, activation='relu')(conv_1)\n",
        "pool_1 = MaxPooling2D(pool_size=(1, pool_size_1))(dense_1)\n",
        "drop_1 = Dropout(drop_prob_1)(pool_1)\n",
        "\n",
        "conv_2 = Convolution2D(conv_depth_2, (1 , kernel_size_1), padding='valid', activation='relu')(drop_1)\n",
        "dense_2 = Dense(conv_depth_2, activation='relu')(conv_2)\n",
        "pool_2 = MaxPooling2D(pool_size=(1, pool_size_2))(dense_2)\n",
        "drop_2 = Dropout(drop_prob_1)(pool_2)\n",
        "\n",
        "conv_3 = Convolution2D(conv_depth_3, (1 , kernel_size_2), padding='valid', activation='relu')(drop_2)\n",
        "drop_3 = Dropout(drop_prob_1)(conv_3)\n",
        "\n",
        "flat = Flatten()(drop_3)\n",
        "hidden = Dense(hidden_size, activation='relu')(flat)\n",
        "drop_4 = Dropout(drop_prob_2)(hidden)\n",
        "\n",
        "out1 = Dense(act_last_layer_dim, activation= act_activation_func, name = \"ACT\")(drop_4)\n",
        "out2 = Dense(gen_last_layer_dim, activation= gen_activation_func, name = \"GEN\")(drop_4)\n",
        "\n",
        "act_gen_model = Model(inputs=inp, outputs=[out1,out2]) \n",
        "\n",
        "act_gen_model.compile(loss=[act_loss_func, gen_loss_func], \n",
        "          optimizer='adam', \n",
        "          metrics=metrics)\n",
        "\n",
        "history = act_gen_model.fit(train_data, [act_train_labels, gen_train_labels],                \n",
        "              batch_size = batch_size,\n",
        "              epochs = num_of_epochs,\n",
        "              verbose = verbosity) \n"
      ],
      "metadata": {
        "id": "Kea7B8BFy8wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_1 = act_gen_model.evaluate(test_data, [act_test_labels, gen_test_labels],\n",
        "                                 verbose = verbosity)\n",
        "\n",
        "print(\"--> Evaluation on Test Dataset:\")\n",
        "print(\"**** Accuracy for Activity Recognition task is: \", results_1[3])\n",
        "print(\"**** Accuracy for Gender Classification task is: \", results_1[4])"
      ],
      "metadata": {
        "id": "bKmuIcfJy932"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## serialize model to JSON and save MTCNN model\n",
        "act_gen_model_json = act_gen_model.to_json()\n",
        "with open(\"act_gen_model_1_ms_t.json\", \"w\") as json_file:\n",
        "    json_file.write(act_gen_model_json)\n",
        "## serialize weights to HDF5 and save learned weights\n",
        "act_gen_model.save_weights(\"act_gen_weights_1_ms_t.h5\")\n",
        "print(\"--> Saved MTCNN and its weights to disk!\")"
      ],
      "metadata": {
        "id": "-wBMp3e8y_DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##***@@@ This Will Be the NEUTRALIZER @@@***##\n",
        "import keras.backend as K\n",
        "def gen_equ_loss_func(y_true, y_pred):\n",
        "    loss = K.mean(K.abs(0.5 - y_pred))\n",
        "    return loss\n",
        "##____________________________________"
      ],
      "metadata": {
        "id": "hhp4O3lczAz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##***@@@ This Will Be the GUARDIAN @@@***##\n",
        "## Here we set up the Autoencoder\n",
        "ae_inp_size = height*width\n",
        "ae_input = Input(shape=(height, width,1))\n",
        "x = Reshape((ae_inp_size,), input_shape=((height,width,1)))(ae_input)\n",
        "x = Dense(ae_inp_size, activation='linear')(x)\n",
        "\n",
        "encoded = Dense(ae_inp_size//2, activation='relu')(x)\n",
        "encoded = Dense(ae_inp_size//4, activation='relu')(encoded)\n",
        "\n",
        "y = Dense(ae_inp_size//8, activation='relu')(encoded)\n",
        "\n",
        "decoded = Dense(ae_inp_size//4, activation='relu')(y)\n",
        "decoded = Dense(ae_inp_size//2, activation='relu')(decoded)\n",
        "\n",
        "z = Dense(ae_inp_size, activation='linear')(decoded)\n",
        "z = Reshape((height,width,1), input_shape=(ae_inp_size,))(z)\n",
        "ae_model = Model(ae_input, z)"
      ],
      "metadata": {
        "id": "v-Gkhp1lzB4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##***@@@ This Will Be the The Final GEN @@@***##\n",
        "## Here we freeze the weights of the MTCNN layers and attach the output of \n",
        "## deep autoencoder to the input of the MTCNN to build the GEN neural network. \n",
        "act_gen_model.trainable = False\n",
        "dp = ae_model(ae_input)\n",
        "dp = act_gen_model(dp)\n",
        "dp_model = Model(inputs=ae_input, outputs=dp)\n",
        "\n",
        "dp_model.compile(loss=[act_loss_func, gen_equ_loss_func], \n",
        "                 optimizer='adam',\n",
        "                 metrics=metrics)\n",
        "\n",
        "num_of_epochs = 20\n",
        "dp_model.fit(train_data , [act_train_labels, gen_train_labels],\n",
        "                epochs = num_of_epochs,\n",
        "                batch_size = batch_size,\n",
        "                )"
      ],
      "metadata": {
        "id": "I0depo9jzDIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_test_data = ae_model.predict(test_data)\n",
        "results_2 = act_gen_model.evaluate(tr_test_data, [act_test_labels, gen_test_labels])\n",
        "print(\"@@@@ Transformed Test ACT acc: \", results_2[3])\n",
        "print(\"@@@@ Transformed Test GEN acc: \", results_2[4])"
      ],
      "metadata": {
        "id": "3dwKywMlzEBZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}